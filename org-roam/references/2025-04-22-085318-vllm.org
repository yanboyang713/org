:PROPERTIES:
:ID:       ac10704e-7f03-4372-a449-0cd3b91500f3
:END:
#+title: vLLM
#+date: 2025-04-22

* Deploying vLLM on [[id:b60301a4-574f-43ee-a864-15f5793ea990][Kubernetes]]
:PROPERTIES:
:ID:       3a56acec-bb1f-4910-adc2-5eab190db964
:END:

https://docs.vllm.ai/en/latest/deployment/k8s.html

* Deploying vLLM on VM
** Pre-requires
+ [[id:5bb91c8f-3c4b-40c0-bb43-7c6bf56e795c][Adding a vGPU to a Proxmox VM]]
+ or [[id:81f3a757-0a43-402b-a418-79ea92e93562][proxmox GPU PCI passthrough]]
  
** VM setting
In VM → Hardware:

Click Display, set to Default (or VirtIO-GPU).

Edit your PCI Device (01:00.0) and UNTICK “Primary GPU”. Keep All Functions + PCI-Express checked.

** Install NVIDIA driver on [[id:803d821b-6f7d-4e07-9a1f-08c9736c7dec][ubuntu]] 
:PROPERTIES:
:ID:       6bc4cb0c-c877-4a84-831f-2d37dd60161e
:END:

Inside the VM:
#+begin_src bash
sudo apt update
sudo apt install -y ubuntu-drivers-common
sudo ubuntu-drivers autoinstall
sudo reboot
# after reboot
nvidia-smi
lspci -nnk | grep -iA3 nvidia
#+end_src

** Install vLLM
#+begin_src bash
sudo apt update
sudo apt install -y python3-venv python3-pip build-essential
python3 -m venv ~/venvs/vllm
source ~/venvs/vllm/bin/activate
python -m pip install -U pip wheel setuptools
#+end_src
** Install PyTorch (CUDA build)
Pick the CUDA 12.x wheel from PyTorch’s selector. Example (CUDA 12.4 wheel—if the site shows cu126/cu128, use that instead):
#+begin_src bash
pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio
#+end_src
** Install vLLM
#+begin_src bash
pip install vllm
#+end_src
** Sanity check
#+begin_src bash
python - <<'PY'
import torch, vllm
print("CUDA available:", torch.cuda.is_available())
print("CUDA reported by PyTorch:", torch.version.cuda)
print("Torch:", torch.__version__)
print("GPU:", torch.cuda.get_device_name(0))
print("vLLM:", vllm.__version__)
PY
#+end_src
** Hugging Face Create a token
Sign in at [[https://huggingface.co/][huggingface.co]] → click your avatar → Settings → Access Tokens → New token.

Name it and choose Role = Read (enough to download models).

Click Create and copy the token (looks like hf_********).
** Use the token on your vLLM
#+begin_src bash
  # in your vLLM Python env
  pip install -U "huggingface_hub[cli]" sentencepiece
  git config --global credential.helper store
  hf auth login          # paste your hf_ token when prompted
  hf auth whoami         # sanity check
#+end_src
** Accept the model terms (once, in browser)
Sign in at Hugging Face with the account you’ll use on the VM.
Open: https://huggingface.co/google/gemma-3-4b-it
Click Agree and access (or Request access) and confirm.
If you might also use the base model, do the same for google/gemma-3-4b

** Deploy Gemma 3 4B on your vLLM VM (RTX 2070, 8 GB)
#+begin_src bash
# one-shot shell
export HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxx...   # (optionally: export HF_TOKEN=$HUGGINGFACE_HUB_TOKEN)
# systemd (recommended)
sudo tee /etc/systemd/system/vllm.env >/dev/null <<'EOF'
HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxx...
HF_HOME=/opt/models/.cache/huggingface
EOF
sudo chmod 600 /etc/systemd/system/vllm.env
# then in /etc/systemd/system/vllm.service under [Service]:
# EnvironmentFile=/etc/systemd/system/vllm.env
sudo systemctl daemon-reload && sudo systemctl restart vllm
#+end_src

#+begin_src bash
HF_HOME=/opt/models/.cache/huggingface \
vllm serve google/gemma-3-270m-it \
  --host 0.0.0.0 --port 8000 \
  --max-model-len 2048 \           # you can raise later (even 4096 fits)
  --max-num-seqs 1 \               # keep concurrency low at first
  --gpu-memory-utilization 0.80 \  # headroom for kernels
  --swap-space 2 \                 # your VM has 10GB RAM; 2GB is safe
  --download-dir /opt/models \
  --trust-remote-code
#+end_src

#+begin_src bash
  sudo mkdir -p /opt/models && sudo chown $USER:$USER /opt/models
  HF_HOME=/opt/models/.cache/huggingface \
  vllm serve google/embeddinggemma-300m \
  --host 0.0.0.0 --port 8000 \
  --download-dir /opt/models
#+end_src

** [[id:f7904304-e3e3-484c-b541-349030a56fe3][firewall]] setting
Open the port if you’ll call it from outside the VM
#+begin_src bash
sudo ufw allow from 192.168.1.0/24 to any port 8000 proto tcp
#+end_src

** Health check
#+begin_src console
alpine-docker:~# curl -i http://192.168.1.243:8000/health
HTTP/1.1 200 OK
date: Mon, 27 Oct 2025 03:47:22 GMT
server: uvicorn
content-length: 0
#+end_src

** Run it as a service
#+begin_src bash
# /etc/systemd/system/vllm-embed.service
[Unit]
Description=vLLM - EmbeddingGemma-300M
After=network-online.target
Wants=network-online.target

[Service]
User=yanboyang713
Environment=HF_HOME=/opt/models/.cache/huggingface
# If the model is gated, add: Environment=HUGGINGFACE_HUB_TOKEN=hf_xxx
ExecStart=/home/yanboyang713/venvs/vllm/bin/vllm serve google/embeddinggemma-300m \
  --task embedding --host 0.0.0.0 --port 8001 --download-dir /opt/models
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target

#+end_src

#+begin_src bash
sudo systemctl daemon-reload
sudo systemctl enable --now vllm-embed
sudo systemctl status vllm-embed --no-pager
#+end_src
