:PROPERTIES:
:ID:       64aa42dc-14c2-48c4-8360-45a31aa73f7f
:END:
#+title: MLFlow

* What is MLflow?
Whether you’re an individual researcher, a member of a large team, or somewhere in between, MLflow provides a unified platform to navigate the intricate maze of model development, deployment, and management. MLflow aims to enable innovation in ML solution development by streamlining otherwise cumbersome logging, organization, and lineage concerns that are unique to model development. This focus allows you to ensure that your ML projects are robust, transparent, and ready for real-world challenges.

* Start MLflow Server
https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html#method-1-start-your-own-mlflow-server
** [Method 1] Start your own MLflow server.
[[id:085d21e2-2710-4378-84cd-05eb860a86f0][start a local MLflow server]]

** [Method 2] Use a free hosted tracking server - Databricks Community Edition.

** [Method 3] Use production Databricks/AzureML.

* [[id:21320dd2-7534-401b-ad71-b11446021da9][MLflow Tracking]](Runs)
* [[id:238de291-6416-4ac7-aeaf-57011e528f1c][MLflow Tracking (datasets)]] 
* [[id:9e0dbde4-42a1-4d4d-9a81-89a0390a5f00][MLflow System Metrics]]
* [[id:7c599534-6ad9-45d6-bfcf-70170fc4dd5e][Hyperparameter Tuning with MLflow and Optuna]]

* Tips
Most of these tricks are parameters of the start_run() function. We call this function to initiate our experiment run, and it becomes the active run where we can log parameters, metrics, and other information.
** *experiment_id*
You can set the experiment you want a run to log to in a few different ways in MLflow. The first command sets the experiment for all subsequent runs to *mlflow_sdk_test*.

#+begin_src python
mlflow.set_experiment("/mlflow_sdk_test")
#+end_src

This can also be configured on a run-by-run basis through the *experiment_id* parameter.
#+begin_src python
my_experiment = mlflow.set_experiment("/mlflow_sdk_test")
experiment_id = my_experiment.experiment_id
#+end_src

This value can then be reused when passed to *start_run()*:
#+begin_src python
# End any existing runs
mlflow.end_run()

with mlflow.start_run(experiment_id=experiment_id):
    # Turn autolog on to save model artifacts, requirements, etc.
    mlflow.autolog(log_models=True)

    print(run.info.run_id)

    diabetes_X = diabetes.data
    diabetes_y = diabetes.target

    # Split data into test training sets, 3:1 ratio
    (
        diabetes_X_train,
        diabetes_X_test,
        diabetes_y_train,
        diabetes_y_test,
    ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)

    alpha = 0.8
    solver = "cholesky"
    regr = linear_model.Ridge(alpha=alpha, solver=solver)

    regr.fit(diabetes_X_train, diabetes_y_train)

    diabetes_y_pred = regr.predict(diabetes_X_test)

    # Log desired metrics
    mlflow.log_metric("mse", mean_squared_error(diabetes_y_test, diabetes_y_pred))
    mlflow.log_metric(
        "rmse", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))
    )
    mlflow.log_metric("r2", r2_score(diabetes_y_test, diabetes_y_pred))
#+end_src

** *run_id*
The *run_id* is a UUID which is specific to each experiment run. Once a run has been initiated, it is not possible to overwrite properties such as the model type or parameter values. However, you can use the *run_id* to log additional values retrospectively, such as metrics, tags, or a description.
#+begin_src python
# Start MLflow run for this experiment

# End any existing runs
mlflow.end_run()

with mlflow.start_run() as run:
    # Turn autolog on to save model artifacts, requirements, etc.
    mlflow.autolog(log_models=True)

    print(run.info.run_id)

    diabetes_X = diabetes.data
    diabetes_y = diabetes.target

    # Split data into test training sets, 3:1 ratio
    (
        diabetes_X_train,
        diabetes_X_test,
        diabetes_y_train,
        diabetes_y_test,
    ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)

    alpha = 0.9
    solver = "cholesky"
    regr = linear_model.Ridge(alpha=alpha, solver=solver)

    regr.fit(diabetes_X_train, diabetes_y_train)

    diabetes_y_pred = regr.predict(diabetes_X_test)

    # Log desired metrics
    mlflow.log_metric("mse", mean_squared_error(diabetes_y_test, diabetes_y_pred))
    mlflow.log_metric(
        "rmse", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))
    )
#+end_src

In this case, we may also want to log our coefficient of determination (r²) value for this run:
#+begin_src python
with mlflow.start_run(run_id="3fcf403e1566422493cd6e625693829d") as run:
    mlflow.log_metric("r2", r2_score(diabetes_y_test, diabetes_y_pred))
#+end_src

The *run_id* can either be extracted by *print(run.info.run_id)* from the previous run, or by querying *mlflow.search_runs()*, but more on that later.

** *run_name*
When you specify the name of your run, you have greater control over the naming process than relying on the default names generated by MLflow. This enables you to establish a consistent naming convention for experiment runs, similar to how you might manage other resources in your environment.
#+begin_src python
# Start MLflow run for this experiment

# End any existing runs
mlflow.end_run()

# Explicitly name runs
today = dt.today()

run_name = "Ridge Regression " + str(today)

with mlflow.start_run(run_name=run_name) as run:
    # Turn autolog on to save model artifacts, requirements, etc.
    mlflow.autolog(log_models=True)

    print(run.info.run_id)

    diabetes_X = diabetes.data
    diabetes_y = diabetes.target

    # Split data into test training sets, 3:1 ratio
    (
        diabetes_X_train,
        diabetes_X_test,
        diabetes_y_train,
        diabetes_y_test,
    ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)

    alpha = 0.5
    solver = "cholesky"
    regr = linear_model.Ridge(alpha=alpha, solver=solver)

    regr.fit(diabetes_X_train, diabetes_y_train)

    diabetes_y_pred = regr.predict(diabetes_X_test)

    # Log desired metrics
    mlflow.log_metric("mse", mean_squared_error(diabetes_y_test, diabetes_y_pred))
    mlflow.log_metric(
        "rmse", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))
    )
    mlflow.log_metric("r2", r2_score(diabetes_y_test, diabetes_y_pred))
#+end_src

However, please be aware that run_name is not a unique constraint in MLflow. This means that you could have multiple experiments (with unique run IDs) sharing the same name.

This means that every time you execute a new run in a with statement, it will create a new experiment of the same name, rather than append details to this run.

** nested
You may be familiar with nested experiment runs if you’ve run the scikit-learn functionGridSearchCV to perform hyperparameter optimisation.

Note that the metrics here are saved against the parent run, which returns the best values recorded by the child runs. The child run values themselves are blank.

While nested experiments are excellent for evaluating and logging parameter combinations to determine the best model, they also serve as a great logical container for organizing your work. With the ability to group experiments, you can compartmentalize individual data science investigations and keep your experiments page organized and tidy.

#+begin_src python
# End any existing runs
mlflow.end_run()

# Explicitly name runs
run_name = "Ridge Regression Nested"

with mlflow.start_run(run_name=run_name) as parent_run:
    print(parent_run.info.run_id)

    with mlflow.start_run(run_name="Child Run: alpha 0.1", nested=True):
        # Turn autolog on to save model artifacts, requirements, etc.
        mlflow.autolog(log_models=True)

        diabetes_X = diabetes.data
        diabetes_y = diabetes.target

        # Split data into test training sets, 3:1 ratio
        (
            diabetes_X_train,
            diabetes_X_test,
            diabetes_y_train,
            diabetes_y_test,
        ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)

        alpha = 0.1
        solver = "cholesky"
        regr = linear_model.Ridge(alpha=alpha, solver=solver)

        regr.fit(diabetes_X_train, diabetes_y_train)

        diabetes_y_pred = regr.predict(diabetes_X_test)

        # Log desired metrics
        mlflow.log_metric("mse", mean_squared_error(diabetes_y_test, diabetes_y_pred))
        mlflow.log_metric(
            "rmse", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))
        )
        mlflow.log_metric("r2", r2_score(diabetes_y_test, diabetes_y_pred))
#+end_src

Should you need to add to this nested run, then specify the parent run’s run_id in subsequent executions as a parameter, appending further child runs.

#+begin_src python
# End any existing runs
mlflow.end_run()

with mlflow.start_run(run_id="61d34b13649c45699e7f05290935747c") as parent_run:
    print(parent_run.info.run_id)
    with mlflow.start_run(run_name="Child Run: alpha 0.2", nested=True):
        # Turn autolog on to save model artifacts, requirements, etc.
        mlflow.autolog(log_models=True)

        diabetes_X = diabetes.data
        diabetes_y = diabetes.target

        # Split data into test training sets, 3:1 ratio
        (
            diabetes_X_train,
            diabetes_X_test,
            diabetes_y_train,
            diabetes_y_test,
        ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)

        alpha = 0.2
        solver = "cholesky"
        regr = linear_model.Ridge(alpha=alpha, solver=solver)

        regr.fit(diabetes_X_train, diabetes_y_train)

        diabetes_y_pred = regr.predict(diabetes_X_test)

        # Log desired metrics
        mlflow.log_metric("mse", mean_squared_error(diabetes_y_test, diabetes_y_pred))
        mlflow.log_metric(
            "rmse", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))
        )
        mlflow.log_metric("r2", r2_score(diabetes_y_test, diabetes_y_pred))
#+end_src

One thing to note about this approach is that your metrics will now be logged against each child run.

** *mlflow.search_runs()*
This trick is using the *search_runs()* function.

This function allows us to programmatically query the experimentation GUI, and the results are returned in a tabular format that is easy to understand and manipulate.

In the below example, we can select specific fields from the runs in our experiment and load them into a Pandas DataFrame. Notice that the available columns greatly exceed those available in the experiments GUI!

#+begin_src python
# Create DataFrame of all runs in *current* experiment
df = mlflow.search_runs(order_by=["start_time DESC"])

# Print a list of the columns available
# print(list(df.columns))

# Create DataFrame with subset of columns
runs_df = df[
    [
        "run_id",
        "experiment_id",
        "status",
        "start_time",
        "metrics.mse",
        "tags.mlflow.source.type",
        "tags.mlflow.user",
        "tags.estimator_name",
        "tags.mlflow.rootRunId",
    ]
].copy()
runs_df.head()
#+end_src

As this is a Pandas DataFrame, we can add columns that may be useful for analysis:
#+begin_src python
# Feature engineering to create some additional columns
runs_df["start_date"] = runs_df["start_time"].dt.date
runs_df["is_nested_parent"] = runs_df[["run_id","tags.mlflow.rootRunId"]].apply(lambda x: 1 if x["run_id"] == x["tags.mlflow.rootRunId"] else 0, axis=1)
runs_df["is_nested_child"] = runs_df[["run_id","tags.mlflow.rootRunId"]].apply(lambda x: 1 if x["tags.mlflow.rootRunId"] is not None and x["run_id"] != x["tags.mlflow.rootRunId"]else 0, axis=1)
runs_df
#+end_src
If we want to aggregate the result set to provide information of runs over time, we can use:
#+begin_src python
pd.DataFrame(runs_df.groupby("start_date")["run_id"].count()).reset_index()
#+end_src

The automatic *tags.estimator_name* field allows us to review how many runs have been tested for each algorithm.
#+begin_src python
pd.DataFrame(runs_df.groupby("tags.estimator_name")["run_id"].count()).reset_index()
#+end_src

Given this is a DataFrame, we can export the data for any reporting requirements to give the required visibility to users who may not have access to the workspace, and compare across workspaces.

https://towardsdatascience.com/5-quick-tips-to-improve-your-mlflow-model-experimentation-dae346db825

* [[id:526c4f39-0de6-4273-a353-beb774585c14][MLflow Recipes]] 
* [[id:2a1dbc6c-bee0-4fae-becd-c8d479915ff8][mlflow simulataneous runs in the experiment]]  

* Introduction To MLflow | Track Your Machine Learning Experiments
https://www.youtube.com/watch?v=ksYIVDue8ak

* Setting Up MLflow Experiments To a Remote Server | DagsHub
https://www.youtube.com/watch?v=K9se7KQON5k

* How To Setup MLflow Experiments with AWS
https://www.youtube.com/watch?v=XEZ7Hx2NrO8

* Deplyment
[[id:e08c11c2-8a7a-4684-86a4-d299733a8694][deploy MLFlow on Kubernetes]]

* Reference List
1. https://mlflow.org/
2. https://www.youtube.com/watch?v=X2XxeLTkv0w
3. https://www.youtube.com/watch?v=X2XxeLTkv0w
4. https://github.com/dmatrix/tmls-workshop
5. https://mlflow.org/docs/latest/introduction/index.html
