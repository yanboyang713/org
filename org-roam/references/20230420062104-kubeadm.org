:PROPERTIES:
:ID:       9d293990-ff98-47eb-93a4-556df1e7b26d
:END:
#+title: kubeadm
#+filetags:

Kubeadm is an excellent tool to set up a working kubernetes cluster in less time. It does all the heavy lifting in terms of setting up all kubernetes cluster components. Also, It follows all the configuration best practices for a kubernetes cluster.

* What is Kubeadm?
:PROPERTIES:
:ID:       b19024ae-742e-4894-bea5-3936f424e80e
:END:
Kubeadm is a tool to set up a minimum viable Kubernetes cluster without much complex configuration. Also, Kubeadm makes the whole process easy by running a series of prechecks to ensure that the server has all the essential components and configs to run Kubernetes.

It is developed and maintained by the official Kubernetes community.

But if you want to play around with the cluster components or test utilities that are part of cluster administration, Kubeadm is the best option. Also, you can create a production-like cluster locally on a workstation for development and testing purposes.

* How Does Kubeadm Work?
:PROPERTIES:
:ID:       7dd25874-7faf-456d-b0d1-d1efec36a945
:END:
When you initialize a Kubernetes cluster using Kubeadm, it does the following.

+ When you initialize kubeadm, first it runs all the preflight checks to validate the system state and it downloads all the required cluster container images from the registry.k8s.io container registry.
+ It then generates required TLS certificates and stores them in the /etc/kubernetes/pki folder.
+ Next, it generates all the kubeconfig files for the cluster components in the /etc/kubernetes folder.
+ Then it starts the kubelet service and generates the static pod manifests for all the cluster components and saves it in the /etc/kubernetes/manifests folder.
+ Next, it starts all the control plane components from the static pod manifests.
+ Then it installs core DNS and Kubeproxy components
+ Finally, it generates the node bootstrap token.
+ Worker nodes use this token to join the control plane.

[[https://devopscube.com/wp-content/uploads/2023/04/image-7.png]]

As you can see all the key cluster configurations will be present under the /etc/kubernetes folder.

* Kubeadm Setup Prerequisites
:PROPERTIES:
:ID:       8f8a45b2-a8b6-4010-99a6-1390bef1b4c5
:END:
Following are the prerequisites for Kubeadm Kubernetes cluster setup.
+ Minimum two Ubuntu nodes [One master and one worker node]. You can have more worker nodes as per your requirement.
+ The master node should have a minimum of 2 vCPU and 2GB RAM.
+ For the worker nodes, a minimum of 1vCPU and 2 GB RAM is recommended.
+ 10.X.X.X/X network range with static IPs for master and worker nodes. Make sure the Node IP range and pod IP range don’t overlap.

*Note*: If you are setting up the cluster in the corporate network behind a proxy, ensure set the proxy variables and have access to the container registry and docker hub. Or talk to your network administrator to whitelist registry.k8s.io to pull the required images.

* Kubeadm Port Requirements
Please refer to the following image and make sure all the ports are allowed for the control plane (master) and the worker nodes. If you are setting up the kubeadm cluster cloud servers, ensure you allow the ports in the firewall configuration.
[[https://devopscube.com/wp-content/uploads/2019/12/kuberetes-port-requirements-min.png.webp]]

* Kubeadm Scripts & Manifests (Auto)
:PROPERTIES:
:ID:       01bd7e0f-833a-4ba2-a962-88fd00a055fd
:END:
Also, all the commands used in this guide for master and worker nodes config are hosted in [[https://github.com/yanboyang713/kubeadm-scripts.git][GitHub]]. You can clone the repository for reference.
#+begin_src bash
git clone https://github.com/yanboyang713/kubeadm-scripts.git
#+end_src

This guide intends to make you understand each config required for the Kubeadm setup. If you don’t want to run the command one by one, you can run the script file directly.

Also, I have created a video demo of the whole kubeadm setup. You can refer it during the setup.
[[https://www.youtube.com/watch?v=xX52dc3u2HU&t=1s&ab_channel=DevopsCube]]

** [[id:dcf5e347-8a8a-4c63-a822-53f558025f8c][AWS]] 
If you are a [[id:508e62ef-40f1-4568-a6c1-bf7654accda9][Terraform]] and AWS user, you can make use of the Terraform script present under the Terraform folder to spin up ec2 instances.
** run script
#+begin_src bash
  # All nodes
  sudo bash ./scripts/common.sh
  # master node only
  bash ./scripts/master.sh
#+end_src

* Kubernetes Cluster Setup Using Kubeadm (Manual)
:PROPERTIES:
:ID:       892ff35f-6bb0-49b4-a0a6-e3e495c55e36
:END:
Following are the high-level steps involved in setting up a kubeadm-based Kubernetes cluster.
+ Install container runtime on all nodes- We will be using [[https://cri-o.io/][cri-o]].
+ Install Kubeadm, Kubelet, and kubectl on all the nodes.
+ Initiate Kubeadm control plane configuration on the master node.
+ Save the node join command with the token.
+ Install the Calico network plugin.
+ Join the worker node to the master node (control plane) using the join command.
+ Validate all cluster components and nodes.
+ Install Kubernetes Metrics Server
+ Deploy a sample app and validate the app

If you want to understand every cluster component in detail, refer to the comprehensive [[id:4bbe34aa-f059-48b1-80c2-d95a96718aaa][Kubernetes Architecture]].

Now let’s get started with the setup.

** Enable iptables Bridged Traffic on all the Nodes
Execute the following commands on all the nodes for IPtables to see bridged traffic.
#+begin_src bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
#+end_src

** Disable [[id:bfe1d26e-95ab-4edd-9f7a-2ca0904d8484][Swap]] on all the Nodes
For kubeadm to work properly, you need to disable swap on all the nodes using the following command.
#+begin_src bash
# Function to comment out lines containing the keyword
comment_lines_with_keyword() {
    local keyword=$1
    local file=$2

    # Create a temporary file
    local temp_file=$(mktemp)

    while IFS= read -r line; do
        if [[ $line == *"$keyword"* ]]; then
            echo "# $line" >> "$temp_file"
        else
            echo "$line" >> "$temp_file"
        fi
    done < "$file"
    # Make file backup
    mv "$file" "$file.backup"
    # Overwrite the original file with the modified content
    mv "$temp_file" "$file"
    echo "Modified content written back to $file"
}

# check swap size, if swap great than 1 bytes, disable Swap permanently
disable_swap_permanently() {
    # Get swap size in bytes
    swap_size=$(grep 'SwapTotal' /proc/meminfo | awk '{print $2}')

    # Check if the variable is less than 1
    if (( $(echo "$swap_size < 1" | bc -l) )); then
        echo "The Swap size is less than 1 bytes."
    else
        # Print the swap size
        echo "Swap Size: $swap_size"
        # disable swap permanently
        comment_lines_with_keyword "swap" "/etc/fstab"
        # apply the new swap setting
        mount -a
        # Disable Swap
        swapoff -a
    fi

}

# Disable Swap
disable_swap(){
    echo "K8S required Disable Swap"
    read -p "Do you want to Disable Swap Permanently? Otherwise Disable Temporarily (yes/no): " answer

    case "$answer" in
        [Yy][Ee][Ss]|[Yy])
            echo "Disable Swap Permanently"
            disable_swap_permanently
            ;;
        [Nn][Oo]|[Nn])
            echo "Disable Swap Temporarily"
            swapoff -a
            ;;
        *)
            echo "Invalid input. Please enter 'yes' or 'no'."
            ;;
    esac
}
#+end_src

The fstab entry will make sure the swap is off on system reboots.

You can also, control swap errors using the kubeadm parameter --ignore-preflight-errors Swap we will look at it in the latter part.

** Install CRI-O Runtime On All The Nodes
The basic requirement for a Kubernetes cluster is a [[https://devopscube.com/what-is-docker/][container runtime]]. You can have any one of the following container runtimes.

+ CRI-O
+ containerd
+ Docker Engine (using cri-dockerd)

We will be using CRI-O instead of Docker for this setup as [[https://kubernetes.io/blog/2022/02/17/dockershim-faq/][Kubernetes deprecated Docker engine]]

As a first step, we need to install cri-o on all the nodes. Execute the following commands on all the nodes.
#+begin_src bash
# Install CRI-O Runtime
install_CRI_O_runtime(){

    dist_id=$(lsb_release -is)   # This gets the distributor ID (e.g., Ubuntu)
    version=$(lsb_release -rs)   # This gets the release version of the OS

    if [ "$dist_id" = "Ubuntu" ]; then
	echo "Operating system is Ubuntu."

	if [ "$version" = "22.04" ]; then
            # Set variable if Ubuntu version is 22.04
	    OS="xUbuntu_22.04"
            echo "Ubuntu version is 22.04."
	elif [ "$version" = "21.10" ]; then
	    OS="xUbuntu_21.10"
            echo "Ubuntu version is 21.10."
	else
            echo "Ubuntu version: $version, is not support"
	fi
    else
	echo "Operating system is not Ubuntu. Detected: $dist_id"
    fi

    VERSION="$(echo $1 | grep -oE '[0-9]+\.[0-9]+')"

    echo $VERSION

    # Add CRI source and key
    echo "deb [signed-by=/usr/share/keyrings/libcontainers-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
    echo "deb [signed-by=/usr/share/keyrings/libcontainers-crio-archive-keyring.gpg] https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list

    mkdir -p /usr/share/keyrings
    curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | gpg --dearmor -o /usr/share/keyrings/libcontainers-archive-keyring.gpg
    curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/Release.key | gpg --dearmor -o /usr/share/keyrings/libcontainers-crio-archive-keyring.gpg

    # Update and install crio and crio-tools.
    apt-get update
    apt-get install -y cri-o cri-o-runc

    # Reload the systemd configurations and enable cri-o.
    systemctl daemon-reload
    systemctl enable crio --now

    echo "CRI runtime installed susccessfully"
}
#+end_src

The cri-tools contain crictl, a CLI utility to interact with the containers created by the contianer runtime. When you use container runtimes other than Docker, you can use the crictl utility to debug containers on the nodes. Also, it is useful in CKS certification where you need to debug containers.

** Install Kubeadm & Kubelet & Kubectl on all Nodes
#+begin_src bash
# Install kubelet, kubectl and Kubeadm
install_kubelet_kubectl_kubeadm(){
    # Install the required dependencies.
    apt-get update
    apt-get install -y apt-transport-https ca-certificates curl
    curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg

    # Add the GPG key and apt repository.
    echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list

    # Update apt and install the latest version of kubelet, kubeadm, and kubectl.
    apt-get update -y
    apt-get install -y kubelet=$1 kubectl=$1 kubeadm=$1

    # Add hold to the packages to prevent upgrades.
    apt-mark hold kubelet kubeadm kubectl

}
#+end_src

*Note*: If you are preparing for Kubernetes certification, install the specific version of kubernetes. For example, the current Kubernetes version for CKA, CKAD and CKS exams is kubernetes version 1.28

You can use the following commands to find the latest versions.
#+begin_src bash
sudo apt update
apt-cache madison kubeadm | tac
#+end_src

Now we have all the required utilities and tools for configuring Kubernetes components using kubeadm.

Add the node IP to *KUBELET_EXTRA_ARGS*.

#+begin_src bash
#Add the node IP to KUBELET_EXTRA_ARGS.
set_node_IP(){
    apt-get update -y
    apt-get install -y jq

    IFACE=$(ip route show to match default | perl -nle 'if ( /dev\s+(\S+)/ ) {print $1}')
    local_ip=$(ip --json a s | jq -r --arg IFACE "$IFACE" '.[] | if .ifname == $IFACE then .addr_info[] | if .family == "inet" then .local else empty end else empty end')

    echo "$IFACE interface with IP: $local_ip"

    printf "KUBELET_EXTRA_ARGS=--node-ip=%s\n" "$local_ip" | tee -a /etc/default/kubelet

}
#+end_src

* Initialize Kubeadm On Master Node To Setup Control Plane
:PROPERTIES:
:ID:       4f16267f-d2ea-44a9-83ed-211e1df5585c
:END:

*NOTE:*
1. Make sure you have correct [[id:8b8a1977-3c04-4c9e-9bb5-1d8b12be1eac][hostname]].
2. Recommand all of [[id:b60301a4-574f-43ee-a864-15f5793ea990][Kubernetes]] nodes have fixed IP, you could follow [[id:9c7c6f9e-d330-437e-b9f7-99b705ba9038][Setting up Static IP address on Ubuntu Server]]

Here you need to consider two options.

+ Master Node with Private [[id:c4fd67f4-f52c-4e9c-a564-ba3a482d4c25][IP]] : If you have nodes with only private IP addresses and the API server would be accessed over the private IP of the master node.
+ Master Node With Public [[id:c4fd67f4-f52c-4e9c-a564-ba3a482d4c25][IP]]: If you are setting up a Kubeadm cluster on Cloud platforms and you need master Api server access over the Public IP of the master node server.

Only the Kubeadm initialization command differs for Public and Private IPs.

Execute the commands in this section only on the master node.
#+begin_src bash
# If you need public access to API server using the servers Public IP adress, change PUBLIC_IP_ACCESS to true.

PUBLIC_IP_ACCESS="false"
NODENAME=$(hostname -s)
POD_CIDR="10.1.0.0/16"

# Pull required images

sudo kubeadm config images pull

# Initialize kubeadm based on PUBLIC_IP_ACCESS

if [[ "$PUBLIC_IP_ACCESS" == "false" ]]; then
    IFACE=$(ip route show to match default | perl -nle 'if ( /dev\s+(\S+)/ ) {print $1}')
    MASTER_PRIVATE_IP=$(ip addr show $IFACE | awk '/inet / {print $2}' | cut -d/ -f1)
    sudo kubeadm init --apiserver-advertise-address="$MASTER_PRIVATE_IP" --apiserver-cert-extra-sans="$MASTER_PRIVATE_IP" --pod-network-cidr="$POD_CIDR" --node-name "$NODENAME" --ignore-preflight-errors Swap

elif [[ "$PUBLIC_IP_ACCESS" == "true" ]]; then

    MASTER_PUBLIC_IP=$(curl ifconfig.me && echo "")
    sudo kubeadm init --control-plane-endpoint="$MASTER_PUBLIC_IP" --apiserver-cert-extra-sans="$MASTER_PUBLIC_IP" --pod-network-cidr="$POD_CIDR" --node-name "$NODENAME" --ignore-preflight-errors Swap

else
    echo "Error: MASTER_PUBLIC_IP has an invalid value: $PUBLIC_IP_ACCESS"
    exit 1
fi
#+end_src

All the other steps are the same as configuring the master node with private IP.

*Note*: You can also pass the kubeadm configs as a file when initializing the cluster. See [[https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file][Kubeadm Init with config file]]

On a successful kubeadm initialization, you should get an output with [[https://devopscube.com/kubernetes-kubeconfig-file/][kubeconfig file]] location and the *join command with the token* as shown below. Copy that and save it to the file. we will need it for *joining the worker node to the master*.
[[https://devopscube.com/wp-content/uploads/2021/05/kubeadm-760x428.png.webp]]

** create the kubeconfig in master
Use the following commands from the output to create the kubeconfig in master so that you can use kubectl to interact with cluster API.

#+begin_src bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#+end_src

** Allow scheduling of pods on Kubernetes master
#+begin_src bash
#!/bin/bash

# Ask the user whether to allow scheduling of pods on the Kubernetes master node
read -p "Do you want to allow scheduling of pods on the Kubernetes master node? (y/n): " answer

# Function to remove taint from master node
allow_scheduling() {
    kubectl taint nodes --all node-role.kubernetes.io/control-plane- 
    echo "Scheduling on the Kubernetes master node is now allowed."
}

# Check user's response
case $answer in
    [Yy]* )
        allow_scheduling
        ;;
    [Nn]* )
        echo "No changes made. Scheduling on the Kubernetes master node remains disallowed."
        ;;
    * )
        echo "Invalid input. Please answer 'y' (yes) or 'n' (no)."
        ;;
esac

#+end_src

** Install [[id:fd2a4c2f-4d5f-43b8-aab8-69b1ae33870e][Helm]]
#+begin_src bash
# install helm
curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm
#+end_src


** Install Network Plugin for Pod Networking
Kubeadm does not configure any [[id:203c4778-9335-4eef-b8ab-5aa39093c5cb][Container Network Interface (CNI)]]. You need to install a network plugin of your choice.
#+begin_src bash
# install Network Plugin
echo "Which Network Plugin do you want to install?"
echo "1. Calico"
echo "2. Flannel"
echo "3. Weave"
echo "4. Cilium"
echo "Please enter your choice (1/2/3/4):"

read choice

case $choice in
    1)
        echo "Installing Calico..."
	# Install Claico Network Plugin Network
	curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

	kubectl apply -f calico.yaml

        ;;
    2)
        echo "Installing Flannel..."
        # Add commands to install Flannel here
        ;;
    3)
        echo "Installing Weave..."
        # Add commands to install Weave here
        ;;
    4)
        echo "Installing  Cilium..."
	# Install the Cilium CLI
	CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
	CLI_ARCH=amd64
	if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
	curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
	sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
	sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
	rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
	# Install Cilium
	cilium install --version 1.14.5
	# Validate the Installation
	cilium status --wait
	cilium connectivity test
        ;;

    *)
        echo "Invalid choice. Exiting."
        exit 1
        ;;
esac
#+end_src

Note: Make sure you execute the kubectl command from where you have configured the kubeconfig file. Either from the master of your workstation with the connectivity to the kubernetes API.

** list all the pods
Now, verify the kubeconfig by executing the following kubectl command to list all the pods in the kube-system namespace.
#+begin_src bash
kubectl get po -n kube-system
#+end_src

You should see the following output. You will see the two Coredns pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a running state
[[https://devopscube.com/wp-content/uploads/2021/05/pods-600x193.png.webp]]

*Note*: You can copy the admin.conf file from the master to your workstation in $HOME/.kube/config location if you want to execute kubectl commands from the workstation. Detail: [[id:4a30a096-d006-4351-bfe4-0b926b81ba17][console host use kubectl interact with k8s cluster API]]

** verify all the cluster component health statuses
You verify all the cluster component health statuses using the following command.
#+begin_src bash
kubectl get --raw='/readyz?verbose'
#+end_src

** cluster info 
You can get the cluster info using the following command.
#+begin_src bash
kubectl cluster-info
#+end_src

* Join Worker Nodes To Kubernetes Master Node
:PROPERTIES:
:ID:       f905a75e-4fd6-434f-a093-1f4e09347d9a
:END:
We have set up all utilities on the worker nodes as well.

Now, let’s join the worker node to the master node using the Kubeadm join command you have got in the output while setting up the master node.

If you missed copying the join command, execute the following command in the master node to recreate the token with the join command.

#+begin_src bash
kubeadm token create --print-join-command
#+end_src

Here is what the command looks like. Use sudo if you running as a normal user. This command performs the [[https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/][TLS bootstrapping]] for the nodes.

#+begin_src bash
sudo kubeadm join 10.128.0.37:6443 --token j4eice.33vgvgyf5cxw4u8i \
    --discovery-token-ca-cert-hash sha256:37f94469b58bcc8f26a4aa44441fb17196a585b37288f85e22475b00c36f1c61
#+end_src

On successful execution, you will see the output saying, “This node has joined the cluster”.
[[https://devopscube.com/wp-content/uploads/2022/09/image-25.png]]

** Test
Now execute the kubectl command from the master node to check if the node is added to the master.
#+begin_src bash
kubectl get nodes
#+end_src

Example output,
#+begin_src console
root@master-node:/home/vagrant# kubectl get nodes
NAME            STATUS   ROLES           AGE     VERSION
master-node     Ready    control-plane   14m     v1.24.6
worker-node01   Ready    <none>          2m13s   v1.24.6
worker-node02   Ready    <none>          2m5s    v1.24.6
#+end_src

In the above command, the ROLE is <none> for the worker nodes. You can add a label to the worker node using the following command. Replace worker-node01 with the hostname of the worker node you want to label.

#+begin_src bash
kubectl label node worker-node01  node-role.kubernetes.io/worker=worker
#+end_src

You can further add more nodes with the same join command.

* Deploy A Sample Nginx Application
:PROPERTIES:
:ID:       ab85c230-dbe0-421d-9014-2b1f47f0e963
:END:
Now that we have all the components to make the cluster and applications work, let’s deploy a sample Nginx application and see if we can access it over a NodePort

Create an Nginx [[https://devopscube.com/kubernetes-deployment-tutorial/][deployment]]. Execute the following directly on the command line. It deploys the pod in the default namespace.

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
EOF
#+end_src

Expose the Nginx deployment on a NodePort 32000
#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 32000
EOF
#+end_src

Check the pod status using the following command.
#+begin_src bash
kubectl get pods
#+end_src

Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort.

http://MASTER-IP:32000/

For example,
[[https://devopscube.com/wp-content/uploads/2021/05/nginx.png]]

* Possible Kubeadm Issues
:PROPERTIES:
:ID:       23f86138-c84a-43eb-9764-a0bdafef9163
:END:
Following are the possible issues you might encounter in the kubeadm setup.

+ Pod Out of memory and CPU: The master node should have a minimum of 2vCPU and 2 GB memory.
+ Nodes cannot connect to Master: Check the firewall between nodes and make sure all the nodes can talk to each other on the required kubernetes ports.
+ Calico Pod Restarts: Sometimes, if you use the same IP range for the node and pod network, Calico pods may not work as expected. So make sure the node and pod IP ranges don’t overlap. Overlapping [[id:c4fd67f4-f52c-4e9c-a564-ba3a482d4c25][IP addresses]] could result in issues for other applications running on the cluster as well.
For other pod errors, check out the [[id:b5d6dbb1-72d7-41c7-9752-389945775249][Troubleshoot Kubernetes Pods]] guide.

If your server doesn’t have a minimum of 2 vCPU, you will get the following error.
#+begin_src bash
[ERROR NumCPU]: the number of available CPUs 1 is less than the required 2
#+end_src

If you use public IP with --apiserver-advertise-address parameter, you will have failed master node components with the following error. To rectify this error, use --control-plane-endpoint parameter with the public IP address.

#+begin_src bash
kubelet-check] Initial timeout of 40s passed.


Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
#+end_src

* Kubernetes Cluster Important Configurations
:PROPERTIES:
:ID:       2c98de79-65cb-4ecc-a460-326f454de285
:END:
Following are the important cluster configurations you should know.

| Configuration                                                                    | Location                     |
|----------------------------------------------------------------------------------+------------------------------|
| Static Pods Location (etcd, api-server, controller manager and scheduler)        | /etc/kubernetes/manifests    |
| TLS Certificates location (kubernetes-ca, etcd-ca and kubernetes-front-proxy-ca) | /etc/kubernetes/pki          |
| Admin Kubeconfig File                                                            | /etc/kubernetes/admin.conf   |
| Kubelet configuration                                                            | /var/lib/kubelet/config.yaml |

* Upgrading Kubeadm Cluster
:PROPERTIES:
:ID:       1661c78c-3889-40e1-9432-5bd84276dcb6
:END:
Using kubeadm you can upgrade the kubernetes cluster for the same version patch or a new version.

Kubeadm upgrade doesn’t introduce any downtime if you upgrade one node at a time.

To do hands-on, please refer to my step-by-step guide on [[id:0e6ec5ad-ab9b-46b5-a391-3d22984f69f4][Kubeadm cluster upgrade]].

* Kubeadm FAQs
:PROPERTIES:
:ID:       05b4bfbc-6447-4c96-bedf-033521d491b4
:END:
** How to use Custom CA Certificates With Kubeadm?
By default, kubeadm creates its own CA certificates. However, if you wish to use custom CA certificates, they should be placed in the /etc/kubernetes/pki folder. When kubeadm is run, it will make use of existing certificates if they are found, and will not overwrite them.

** How to generate the Kubeadm Join command?
You can use kubeadm token create --print-join-command command to generate the join command.

* Reference List
1. https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
2. https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
3. https://www.youtube.com/watch?v=j5rmtgyP8vY&ab_channel=%E9%BA%A6%E5%85%9C%E6%90%9EIT
4. https://www.youtube.com/watch?v=u4-Hp-Zxhck&ab_channel=CloudGuru
5. https://github.com/mialeevs/kubernetes_installation_docker
6. https://github.com/techiescamp/kubeadm-scripts
