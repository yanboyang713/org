:PROPERTIES:
:ID:       9d081b8c-6810-430e-af79-5ff425201314
:END:
#+title: neural network interpretability
#+filetags:  

[[id:a40e3787-6e62-4176-80ae-56b9af015ddb][deep neural network]]

Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people’s trust on deep learning systems.

deep learning still has some significant disadvantages. As a really complicated model with millions of free parameters (e.g., AlexNet [2], 62 million), DNNs are often found to exhibit unexpected behaviours.

To open the black-boxes of deep networks, many researchers started to focus on the model interpretability.

One previous definition of interpretability is the ability to provide explanations in understandable terms to a human [18],
+ *Explanations*, ideally, should be logical decision rules(ifthen rules) or can be transformed to logical rules. However, people usually do not require explanations to be explicitly in a rule form (but only some key elements which can be used to construct explanations).
+ *Understandable terms* should be from the domain knowledge related to the task (or common knowledge according to the task).

Although deep networks have shown great performance on some relatively large test sets, the real world environment is still much more complex. As some unexpected failures are inevitable, we need some means of making sure we are still in control.

There are *TWO* categorical and has two possible values, passive interpretation and active interpretability intervention.



* Reference List
1. https://yzhang-gh.github.io/notes/ml/nn-interpretability.html#%E5%89%8D%E8%A8%80
2. Zhang, Y., Tiňo, P., Leonardis, A., & Tang, K. (2021). A survey on neural network interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence, 5(5), 726-742.
